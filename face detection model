import os
import time
import uuid
import cv2
import tensorflow as tf
import json
import numpy as np
import albumentations as alb
from tensorflow.keras.layers import Input, Dense, GlobalMaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.applications import VGG16


IM = os.path.join('data', 'images')
os.makedirs(IM, exist_ok=True)  

cap = cv2.VideoCapture(1)
for i in range(30):
    ret, frame = cap.read()
    filename = os.path.join(IM, f'{str(uuid.uuid1())}.jpg')
    cv2.imwrite(filename, frame)
    cv2.imshow('frame', frame)
    time.sleep(0.5)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()

def load_image(path):
    return tf.io.decode_jpeg(tf.io.read_file(path))

# 2. Prepare labels in train/test/val
for split in ['train', 'test', 'val']:
    for img_name in os.listdir(os.path.join('data', split, 'images')):
        filename = img_name.split('.')[0] + '.json'
        src_label = os.path.join('data', 'labels', filename)
        if os.path.exists(src_label):
            dst_label = os.path.join('data', split, 'labels', filename)
            os.replace(src_label, dst_label)

# 3. Augmentation setup
augmentor = alb.Compose([
    alb.RandomCrop(450, 450),
    alb.HorizontalFlip(0.5),
    alb.RandomBrightnessContrast(0.2),
    alb.RandomGamma(0.2),
    alb.RGBShift(0.2),
    alb.VerticalFlip(0.5)
], bbox_params=alb.BboxParams(format='albumentations', label_fields=['cls']))

# 4. Augment & build augmented data
for i in ['train', 'test', 'val']:
    aug_images_dir = os.path.join('aug_data', i, 'images')
    aug_labels_dir = os.path.join('aug_data', i, 'labels')
    os.makedirs(aug_images_dir, exist_ok=True)
    os.makedirs(aug_labels_dir, exist_ok=True)

    images_dir = os.path.join('data', i, 'images')
    labels_dir = os.path.join('data', i, 'labels')

    for j in os.listdir(images_dir):
        image_path = os.path.join(images_dir, j)
        image = cv2.imread(image_path)
        coords = [0, 0, 0.00001, 0.00001]
        label_path = os.path.join(labels_dir, j[:-4] + '.json')
        if os.path.exists(label_path):
            data = json.load(open(label_path))
            points = data['shapes'][0]['points']
            coords = [points[0][0] / 640, points[0][1] / 480, points[1][0] / 640, points[1][1] / 480]
        for n in range(60):
            augmented = augmentor(image=image, bboxes=[coords], cls=['face'])
            base_name, ext = os.path.splitext(img_name)
            aug_img_name = base_name + '.' + str(n) + ext
            aug_img_path = os.path.join('aug_data', split, 'images', aug_img_name)
            cv2.imwrite(aug_img_path, augmented['image'])
            annotation = {
                'image': img_name,
                'bbox': ([0, 0, 0, 0] if not augmented['bboxes'] else augmented['bboxes'][0]),
                'class': 0 if not augmented['bboxes'] else 1
            }
            with open(os.path.join(aug_labels_dir, f"{img_name[:-4]}.{i}.json"), 'w') as f:
                json.dump(annotation, f)

# 5. Dataset preparation
def prep_images(ima):
    return tf.data.Dataset.list_files(ima).map(load_image).map(lambda x: tf.image.resize(x, (120, 120)) / 255.0)

def prep_labels(path_pattern):
    def parse_json(path):
        path = path.numpy().decode()
        data = json.load(open(path))
        return np.array([data['class']], dtype=np.uint8), np.array(data['bbox'], dtype=np.float32)
    return tf.data.Dataset.list_files(path_pattern).map(
        lambda x: tf.py_function(parse_json, [x], [tf.uint8, tf.float32])
    )

train_ds = tf.data.Dataset.zip((prep_images(os.path.join('aug_data', 'train', 'images', '*.jpg')),
                               prep_labels(os.path.join('aug_data', 'train', 'labels', '*.json'))))
train_ds = train_ds.shuffle(5000).batch(8).prefetch(tf.data.AUTOTUNE)

test_ds = tf.data.Dataset.zip((prep_images(os.path.join('aug_data', 'test', 'images', '*.jpg')),
                              prep_labels(os.path.join('aug_data', 'test', 'labels', '*.json'))))
test_ds = test_ds.shuffle(1300).batch(8).prefetch(tf.data.AUTOTUNE)

val_ds = tf.data.Dataset.zip((prep_images(os.path.join('aug_data', 'val', 'images', '*.jpg')),
                             prep_labels(os.path.join('aug_data', 'val', 'labels', '*.json'))))
val_ds = val_ds.shuffle(1000).batch(8).prefetch(tf.data.AUTOTUNE)

# 6. Build base model
def build_model():
    inp = Input(shape=(120, 120, 3))
    vgg = VGG16(include_top=False)(inp)
    f1 = GlobalMaxPooling2D()(vgg)
    f2 = GlobalMaxPooling2D()(vgg)
    class_out = Dense(1, activation='sigmoid')(Dense(2048, activation='relu')(f1))
    bbox_out = Dense(4, activation='sigmoid')(Dense(2048, activation='relu')(f2))
    return Model(inputs=inp, outputs=[class_out, bbox_out])

# Custom localization loss for bounding box regression
def localization_loss(y_true, y_pred):
    return tf.reduce_mean(tf.abs(y_true - y_pred))

# 7. Custom FaceTracker Model
class FaceTracker(tf.keras.Model):
    def __init__(self, base_model):
        super().__init__()
        self.model = base_model
        self.loss_fn_cls = tf.keras.losses.BinaryCrossentropy()
        self.loss_fn_bbox = localization_loss

    def call(self, inputs):
        return self.model(inputs)

    def train_step(self, data):
        images, (labels_cls, labels_bbox) = data
        with tf.GradientTape() as tape:
            preds_cls, preds_bbox = self(images, training=True)
            loss_cls = self.loss_fn_cls(labels_cls, preds_cls)
            loss_bbox = self.loss_fn_bbox(labels_bbox, preds_bbox)
            loss = loss_cls + loss_bbox
        grads = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        return {'loss': loss, 'class_loss': loss_cls, 'bbox_loss': loss_bbox}

    def test_step(self, data):
        images, (labels_cls, labels_bbox) = data
        preds_cls, preds_bbox = self(images, training=False)
        loss_cls = self.loss_fn_cls(labels_cls, preds_cls)
        loss_bbox = self.loss_fn_bbox(labels_bbox, preds_bbox)
        loss = loss_cls + loss_bbox
        return {'loss': loss, 'class_loss': loss_cls, 'bbox_loss': loss_bbox}

# 8. Compile and train
model = FaceTracker(build_model())
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer)
model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='logs')])
model.model.save('facetracker.h5')

# 9. Inference
model = load_model('facetracker.h5', compile=False)
cap = cv2.VideoCapture(1)
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    roi = frame[50:500, 50:500]
    rgb_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
    input_img = tf.image.resize(rgb_roi, (120, 120)) / 255.0
    preds = model.predict(np.expand_dims(input_img, axis=0))
    class_pred, bbox_pred = preds
    if class_pred[0] > 0.5:
        # Use bbox_pred to draw bounding box (scaled back to roi size)
        h, w = roi.shape[:2]
        x1 = int(bbox_pred[0][0] * w)
        y1 = int(bbox_pred[0][1] * h)
        x2 = int(bbox_pred[0][2] * w)
        y2 = int(bbox_pred[0][3] * h)
        cv2.rectangle(frame, (x1 + 50, y1 + 50), (x2 + 50, y2 + 50), (0, 255, 0), 2)
    cv2.imshow('EyeTrack', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()
